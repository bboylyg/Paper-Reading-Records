[TOC]  

## 1.Adversarial-Based Knowledge Distillation for Multi-Model Ensemble and Noisy Data Refinement(2019)  
*author*: Zhiqiang Shen, Zhankui He, Wanyun Cui, Jiahui Yu, Yutong Zheng, Chenchen Zhu, Marios Savvides    
*link*: https://arxiv.org/pdf/1908.08520  

**outline**  
本文作者提出了将多个模型蒸馏成为一个模型的框架。亮点在于，对教师模型和学生模型的中间输出使用由多层感知机组成的判别器进行对抗训练匹配。  
![](https://github.com/bboylyg/Paper-Reading-Records/blob/master/attention%20transfer%20pics/1.png)  
**loss function**  
$$
\mathcal{L}=\alpha \mathcal{L}_{S i m}+\beta \mathcal{L}_{G A N}
$$
总体损失包括两部分：    
（1）similarity distance between its *output and the soft label* generated by the teacher network. （使用L1,L2以及KL散度衡量）    
（2）the adversarial loss of multi-Stage discriminators   
**Review**  
创新在于引入判别器损失函数，实验结果达到了state-of-art，整体流图可以在知识迁移中借鉴，其他意义不大。

----------------------------------------------------------------------------------------------------  

## 2.Private Model Compression via Knowledge Distillation(2018)  
*author*: Ji Wang1, Weidong Bao1, Lichao Sun2, Xiaomin Zhu13, Bokai Cao4, Philip S. Yu  
*link*: https://arxiv.org/pdf/1908.08520   

**outline**  
本文作者提出一种隐私保护的模型压缩方法。  
>The sensitive data is only used to train the complex teacher model which is not released to the public. It is obvious that the sensitive data are insulated from the explicit invasion of privacy as the student model has no access to it. Further, the information generated by the teacher model, i.e., the hint loss and the distillation loss, are perturbed by additional noises. All the information relating to the sensitive data and the teacher model are isolated or well protected before releasing.  
![](https://github.com/bboylyg/Paper-Reading-Records/blob/master/attention%20transfer%20pics/2.png)  

- 教师在私有数据上训练，学生在公有数据训练  
- 学生通过query samples对教师数据进行请求访问  
- bound&perturb用来对查询结果加入差分隐私扰动，作者称这能提高隐私性  

**loss function**   
（1）Hint learning loss. （L2衡量）    
（2）the distill of soften probabilities loss(KD)  
（3）self learning process loss(local training loss: public samples and the ground truth labels)  
**Review**  
文章提出的私有数据到共有数据的转移，idea很好。但是实验代码没有放出来，对于结果的真实性和效果，有待考证。  

-----------------------------------------------------------------------------------------------

## 3.Be Your Own Teacher, Improve the Performance of Convolutional Neural Networks via Self Distillation(2019 ICCV)  
*author*: Linfeng Zhang, et al.  
*link*: https://arxiv.org/abs/1905.08094  
*开源代码*： https://github.com/luanyunteng/pytorch-be-your-own-teacher  

**outline**  

作者提出一种新的 self-distillation 的策略，用网络深层的特征和预测结果去监督浅层网络的学习。  

![](https://github.com/bboylyg/Paper-Reading-Records/blob/master/attention%20transfer%20pics/3.png)  

**Method**   

作者的方法比较直观简单，如下框图，作者共提出了三种监督信息，全都是从网络深层传至网络浅层，网络共分为四块，每一块得到一个特征图，特征图后接全连接网络和 softmax 得到预测结果，作者称其为浅层 classifier 和深层 classifier。三种监督信息分别是

- 特征监督，深层的特征图对比浅层特征图。
- 标签监督，网络最后 softmax 输出的软标签对比前面 3 个浅层分类器得到的软标签。
- 真实数据监督，one-hot 对比各个分类器的输出。
整体的 loss 如下，C 为分类器的个数，即将网络切分的个数。  

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>l</mi>
  <mi>o</mi>
  <mi>s</mi>
  <mi>s</mi>
  <mo>=</mo>
  <munderover>
    <mo>&#x2211;<!-- ∑ --></mo>
    <mrow class="MJX-TeXAtom-ORD">
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
    </mrow>
    <mi>C</mi>
  </munderover>
  <mo stretchy="false">(</mo>
  <mn>1</mn>
  <mo>&#x2212;<!-- − --></mo>
  <mi>&#x03B1;<!-- α --></mi>
  <mo stretchy="false">)</mo>
  <mo>&#x22C5;<!-- ⋅ --></mo>
  <mi>C</mi>
  <mi>r</mi>
  <mi>o</mi>
  <mi>s</mi>
  <mi>s</mi>
  <mi>E</mi>
  <mi>n</mi>
  <mi>t</mi>
  <mi>r</mi>
  <mi>o</mi>
  <mi>p</mi>
  <mi>y</mi>
  <mo stretchy="false">(</mo>
  <msup>
    <mi>q</mi>
    <mi>i</mi>
  </msup>
  <mo>,</mo>
  <mi>y</mi>
  <mo stretchy="false">)</mo>
  <mo>+</mo>
  <mi>&#x03B1;<!-- α --></mi>
  <mo>&#x22C5;<!-- ⋅ --></mo>
  <mi>K</mi>
  <mi>L</mi>
  <mo stretchy="false">(</mo>
  <msup>
    <mi>q</mi>
    <mi>i</mi>
  </msup>
  <mo>,</mo>
  <msup>
    <mi>q</mi>
    <mi>C</mi>
  </msup>
  <mo stretchy="false">)</mo>
  <mo>+</mo>
  <mi>&#x03BB;<!-- λ --></mi>
  <mo>&#x22C5;<!-- ⋅ --></mo>
  <mrow class="MJX-TeXAtom-ORD">
    <mo stretchy="false">|</mo>
  </mrow>
  <mrow class="MJX-TeXAtom-ORD">
    <mo stretchy="false">|</mo>
  </mrow>
  <msub>
    <mi>f</mi>
    <mi>i</mi>
  </msub>
  <mo>&#x2212;<!-- − --></mo>
  <msub>
    <mi>F</mi>
    <mi>C</mi>
  </msub>
  <mrow class="MJX-TeXAtom-ORD">
    <mo stretchy="false">|</mo>
  </mrow>
  <msubsup>
    <mrow class="MJX-TeXAtom-ORD">
      <mo stretchy="false">|</mo>
    </mrow>
    <mn>2</mn>
    <mn>2</mn>
  </msubsup>
</math>  
**Review**  
在cifar10和cifar100上做了实验。  


------------------------------------------------------------------------
