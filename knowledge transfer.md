1.Adversarial-Based Knowledge Distillation for Multi-Model Ensemble and Noisy Data Refinement(2019)  
author:Zhiqiang Shen, Zhankui He, Wanyun Cui, Jiahui Yu, Yutong Zheng, Chenchen Zhu, Marios Savvides    
link:https://arxiv.org/pdf/1908.08520  

**outline**  
本文作者提出了将多个模型蒸馏成为一个模型的框架。亮点在于，对教师模型和学生模型的中间输出使用由多层感知机组成的判别器进行对抗训练匹配。  
![](https://github.com/bboylyg/Paper-Reading-Records/blob/master/attention%20transfer%20pics/1.png)  
**loss function**  
$$
\mathcal{L}=\alpha \mathcal{L}_{S i m}+\beta \mathcal{L}_{G A N}
$$
总体损失包括两部分：    
（1）similarity distance between its *output and the soft label* generated by the teacher network. （使用L1,L2以及KL散度衡量）    
（2）the adversarial loss of multi-Stage discriminators   
**Review**  
创新在于引入判别器损失函数，实验结果达到了state-of-art，整体流图可以在知识迁移中借鉴，其他意义不大。


