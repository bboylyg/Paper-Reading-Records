1.Adversarial-Based Knowledge Distillation for Multi-Model Ensemble and Noisy Data Refinement(2019)  
*author*: Zhiqiang Shen, Zhankui He, Wanyun Cui, Jiahui Yu, Yutong Zheng, Chenchen Zhu, Marios Savvides    
*link*: https://arxiv.org/pdf/1908.08520  

**outline**  
本文作者提出了将多个模型蒸馏成为一个模型的框架。亮点在于，对教师模型和学生模型的中间输出使用由多层感知机组成的判别器进行对抗训练匹配。  
![](https://github.com/bboylyg/Paper-Reading-Records/blob/master/attention%20transfer%20pics/1.png)  
**loss function**  
$$
\mathcal{L}=\alpha \mathcal{L}_{S i m}+\beta \mathcal{L}_{G A N}
$$
总体损失包括两部分：    
（1）similarity distance between its *output and the soft label* generated by the teacher network. （使用L1,L2以及KL散度衡量）    
（2）the adversarial loss of multi-Stage discriminators   
**Review**  
创新在于引入判别器损失函数，实验结果达到了state-of-art，整体流图可以在知识迁移中借鉴，其他意义不大。

----------------------------------------------------------------------------------------------------  

2.Private Model Compression via Knowledge Distillation(2018)  
*author*: Ji Wang1, Weidong Bao1, Lichao Sun2, Xiaomin Zhu13, Bokai Cao4, Philip S. Yu  
*link*: https://arxiv.org/pdf/1908.08520  

**outline**  
本文作者提出一种隐私保护的模型压缩方法。  
>The sensitive data is only used to train the complex teacher model which is not released to the public. It is obvious that the >sensitive data are insulated from the explicit invasion of privacy as the student model has no access to it. Further, the information >generated by the teacher model, i.e., the hint loss and the distillation loss, are perturbed by additional noises. All the information >relating to the sensitive data and the teacher model are isolated or well protected before releasing.  
![](https://github.com/bboylyg/Paper-Reading-Records/blob/master/attention%20transfer%20pics/2.png)  

- 教师在私有数据上训练，学生在公有数据训练  
- 学生通过query samples对教师数据进行请求访问  
- bound&perturb用来对查询结果加入差分隐私扰动，作者称这能提高隐私性  

**loss function**   
（1）Hint learning loss. （L2衡量）    
（2）the distill of soften probabilities loss(KD)  
（3）self learning process loss(local training loss: public samples and the ground truth labels)  
**Review**  
文章提出的私有数据到共有数据的转移，idea很好。但是实验代码没有放出来，对于结果的真实性和效果，有待考证。  
